<!DOCTYPE html>
<html lang="en">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <title>Human-in-the-Loop Robot Learning: Teaching, Correcting, and Adapting</title>
  <link rel="stylesheet" href="style.css">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L553BZYNH0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L553BZYNH0');
</script>

<body>
  <div class="nav">
    <div class="nav-container">
      <img src="./assets/rss_logo.png" width="200px" style="margin-right: 30px;">
      <a href="#">Home</a>
      <a href="#intro">Introduction</a>
      <a href="#call">Call for Papers</a>
      <!-- <a href="index.html#challenge">Challenge</a> -->
      <a href="#schedule">Schedule</a>
      <a href="#speakers">Speakers</a>
      <a href="#organizers">Organizers</a>
    </div>
  </div>

  <div class="title-container">
    <div style="text-align: center;" class="title-text">
      <div style="width: 90%; margin: auto;z-index: 2; position: relative;">
      <div class="subtitle">RSS 2025 Workshop</div>
      <h2><b>Human-in-the-Loop Robot Learning:<br>Teaching, Correcting, and Adapting</b></h1>
      <div class="subtitle" style="margin: 20px;">
        June 25, 2025, <a href="https://maps.app.goo.gl/nbvsSPvL2U6VfNi47" style="color: white;">OHE</a> 100D
      </div>
      </div>
    </div>
</div>

  <!-- <div class="title-container">
    <div style="text-align: center;" class="title-text">
      <div style="width: 90%; margin: auto;z-index: 2; position: relative;">
      <div class="subtitle">The first CVPR workshop on</div>
      <h1> <b>3D Vision Language Model for Robotics
        Manipulation: Opportunities and Challenges </b></h1>
      <div class="subtitle" style=" margin: 20px">
        June 18, 2024, Seattle, WA. Room: Summit 434.
      </div>
      </div>
    </div>
  </div> -->

  <div class="container">
    <div class="section" id="intro">
      <h2>Introduction</h2>

      <p>

        Modern robots, from assistive and social robots to self-driving cars, are increasingly operating in human-centered environments, 
        requiring them to collaborate more closely with human users than ever before. Beyond performing useful tasks, these robots must 
        also learn and adapt through human interactions to become more effective and intuitive partners. Designing robotic agents that 
        can work alongside, learn from, and adapt to humans is a key research challenge, requiring solutions that address diverse interaction 
        types—including demonstrations, corrections, natural language commands, and preference-based feedback. Another challenge lies in 
        how robots interpret and respond to human input, which can be ambiguous, noisy, and vary across users. Robots must be able to 
        adapt to different interaction styles and evolving user needs while maintaining robust and sample-efficient learning mechanisms. 
        Unlike traditional machine learning settings, where large-scale datasets can be collected and labeled offline, human-in-the-loop 
        learning often requires real-time adaptation with limited supervision, making efficient use of human guidance a critical challenge.

      <!-- <ul>
        <li> <b>High-level vs. Low-level Representations:</b> Is 3D vision crucial, or
          can 2D representations suffice for robotic tasks? How should robots inter-
          pret the world—through point clouds, 3D bounding boxes, or other output
          formats? What input modalities offer the most efficiency and generaliza-
          tion?
        </li>
        <li> <b>Pretraining for Policy Learning:</b> Do low-level policies require exten-
          sive pretraining, or could projections from vision-language models serve
          as adequate features? Is it possible that distilled 2D features are sufficient
          for policy learning, or is a deeper, 3D-centric approach needed? </li>
        <li> <b>3D Vision-Language Action Models:</b> What are the specific challenges
          in using 3D VLMs for robotic actions, particularly regarding sensor cali-
          bration and real-time performance? </li>
      </ul> -->

      <!-- We are bringing together a diverse group of leading experts in the field to present their latest research 
      findings and future perspectives, with an emphasis on scalable, generalizable, and adaptable 3D VLM frameworks for Robotics. 
      Our goal is for this workshop to inspire and drive future innovations in 3D foundation models specifically realting to the robotics manipulation domain. -->
      </p>
      <p>      
        Addressing these challenges demands interdisciplinary collaboration across fields such as machine learning, cognitive science, and 
        control theory. This workshop aims to bring together researchers from robotics, artificial intelligence, machine learning, and human-computer 
        interaction to explore various approaches to human-in-the-loop robot learning. As robots are increasingly deployed in real-world environments, 
        it is more crucial than ever for these agents to be able to correct mistakes on the fly and rapidly personalize their behavior to different 
        users and scenarios. Through discussions on methodologies, benchmarks, and real-world applications, we aim to identify open problems and 
        future directions in human-interactive robot learning.
      </p>


    </div>

    <div class="section" id="call">      
      <h2>Accepted Papers</h2>
      <ul>
          <li><a href="/pdfs/r2bc.pdf">R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations</a> <br/><b>Authors:</b> Connor Mattson, Daniel S. Brown</li>
          <li><a href="">DEXOS: Hand Exoskeleton System for Learning Robot Dexterous Manipulation from Human In-The-Wild</a> <br/><b>Authors:</b> Hao-Shu Fang, Arthur Hu, Branden Romero, Edward H Adelson, Pulkit Agrawal</li>
          <li><a href="/pdfs/compliant.pdf">Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections</a> <br/><b>Authors:</b> Xiaomeng Xu, Yifan Hou, Zeyi Liu, Shuran Song </li>
          <li><a href="/pdfs/uncertainty.pdf">Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion Models</a> <br/><b>Authors:</b> Zhanpeng He, Yifeng Cao, Matei Ciocarlie </li>
          <li><a href="/pdfs/hi_robot.pdf">Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models</a> <br/><b>Authors:</b> Lucy Xiaoyang Shi, brian ichter, Michael Robert Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, Chelsea Finn</li>
          <li><a href="/pdfs/rosetta.pdf">ROSETTA: Constructing Code-Based Reward from Unconstrained Language Preference</a> <br/><b>Authors:</b> Sanjana Srivastava, Kangrui Wang, Yung-Chieh Chan, Tianyuan Dai, Manling Li, Ruohan Zhang, Mengdi Xu, Jiajun Wu, Li Fei-Fei</li>
          <li><a href="/pdfs/creste.pdf">CREStE: Scalable Mapless Navigation with Internet Scale Priors and Counterfactual Guidance</a> <br/><b>Authors:</b> Arthur Zhang, Harshit Sikchi, Amy Zhang, Joydeep Biswas</li>
          <li><a href="/pdfs/masked.pdf">Masked Inverse Reinforcement Learning for Language Conditioned Reward Learning</a> <br/><b>Authors:</b> Minyoung Hwang, Alexandra Forsey-Smerek, Andreea Bobu</li>
          <li><a href="/pdfs/learner.pdf">Learner and Teacher Perspectives on Learning Rewards from Multiple Types of Human Feedback</a> <br/><b>Authors:</b> Ali Larian, Atharv Belsare, Zifan Wu, Daniel S. Brown</li>
          <li><a href="/pdfs/grim.pdf">GRIM: Task-Oriented Grasping with Conditioning on Generative Examples</a> <br/><b>Authors:</b> Shailesh, Alok Raj, Nayan Kumar, Priya Shukla, Andrew Melnik, Michael Beetz, Gora Chand Nandi</li>
          <li><a href="/pdfs/cred.pdf">CRED: Counterfactual Reasoning and Environment Design for Active Preference Learning</a> <br/><b>Authors:</b> Yi-Shiuan Tung, Bradley Hayes, Alessandro Roncone</li>
          <li><a href="/pdfs/interpretable.pdf">Interpretable Human-in-the-Loop In-Context Preference Learning Via Preference Boundaries</a> <br/><b>Authors:</b> Valerie K. Chen, Julie Shah, Andreea Bobu</li>
          <li><a href="/pdfs/causally.pdf">Causally Robust Preference Learning with Reasons</a> <br/><b>Authors:</b> Minjune Hwang, Yigit Korkmaz, Daniel Seita, Erdem Biyik</li>
          <li><a href="/pdfs/hand.pdf">HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval</a> <br/><b>Authors:</b> Matthew Hong, Anthony Liang, Kevin J Kim, Harshitha Belagavi Rajaprakash, Jesse Thomason, Erdem Biyik, Jesse Zhang</li>
          <li><a href="/pdfs/explainable.pdf">Concept Generation through Vision-Language Preference Learning for Understanding Neural Networks’ Internal Representations</a> <br/><b>Authors:</b> Aditya Taparia, Som Sagar, Ransalu Senanayake </li>
        </ul>

      <!-- <h2>Call for Papers</h2>
      <p>
        We are excited to announce the Call for Papers for the workshop. 
        We invite original contributions presenting novel ideas, research, and applications relevant to the workshop's theme.
      </p>

      <h3>Important Dates</h3>
      <table>
        <thead>
          <tr>
            <th>Event</th>
            <th>Date</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Call for Papers</strong></td>
            <td>March 27th, 2025</td>
          </tr>
          <tr>
            <td><strong>Submission Deadline</strong></td>
            <td>May 22th, 2025, 11:59 PM PDT</td>
          </tr>
          <tr>
            <td><strong>Notification</strong></td>
            <td>June 9th, 2025</td>
          </tr>
          <tr>
            <td><strong>Camera-Ready</strong></td>
            <td>June 15th, 2025</td>
          </tr>
        </tbody>
      </table>
      <ul></ul>  -->

      <!-- <h3>Submission Guidelines</h3>
      <p>
        The submissions must use the RSS template, which is available either in <a href="https://roboticsconference.org/docs/paper-template-latex.tar.gz">LaTeX</a> 
        or <a href="https://roboticsconference.org/docs/paper-template-word.zip">Word</a> format. The recommended paper 
        length is 4 pages excluding references. However, any paper that is between 2 and 6 pages, again excluding references, 
        will be reviewed for inclusion in the workshop program. There will be no archival proceedings and the authors of the 
        accepted papers will be given a chance to opt in or out of having their papers on the workshop website. Authors will 
        be asked to present their papers at the workshop in the form of posters. In addition to the posters, we are inviting 
        authors to give a 5-minute spotlight talk to briefly present their work during the workshop.
      </p>
      <p>
        <strong>Submission Link:</strong> <a href="https://openreview.net/group?id=roboticsfoundation.org/RSS/2025/Workshop/HitLRL">OpenReview Link</a>
      </p>
  
      <h3>Paper topics</h3>
      <p>
      A non-exhaustive list of relevant topics:
      <ul>
        <li>Interactive and online learning from human corrections</li>
        <li>Sample-efficient robot learning through human guidance</li>
        <li>Human-aware decision-making and policy adaptation in robotics</li>
        <li>Personalization and user-specific adaptation in robot learning</li>
        <li>Human trust and interpretability in interactive learning systems</li>
        <li>Human-in-the-loop learning in assistive and social robotics</li>
        <li>Multi-agent systems with human-robot interactions</li>
        <li>Learning from multimodal human feedback</li>
      </ul>
      </p> -->
    </div>

    <div class="section" id="schedule">
      <h2>Workshop Schedule</h2>
      <style type="text/css">
        .tg .tg-u4qn {
          background-color: #D9D9D9;
          text-align: left;
          vertical-align: bottom
        }

        .tg-za14 {
          width: 600px;
        }

        /* .tg .tg-7zrl{text-align:left;vertical-align:bottom} */
      </style>
      <table class="tg">
        <thead>
          <tr>
            <th class="tg-u4qn"><span style="background-color:#D9D9D9">Start Time (PDT)</span></th>
            <th class="tg-u4qn"><span style="background-color:#D9D9D9">End Time (PDT)</span></th>
            <th class="tg-u4qn"><span style="background-color:#D9D9D9">Event</span></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="tg-jkyp">8:20 AM</td>
            <td class="tg-jkyp">8:30 AM</td>
            <td class="tg-za14"><b>Opening Remarks</b></td>
          </tr>

          <tr>
            <td class="tg-jkyp">8:30 AM</td>
            <td class="tg-jkyp">09.50 AM</td>
            <td class="tg-za14"><b>Invited Speakers: Part 1 </b> <br/>
              <div class="talk-container">
                <div class="talk-title" onclick="toggleAbstract('talk1')"><b>Luka Peternel</b> - Incorporating Human Physical Behaviour into Robot Control Loop for Adaptive Human-Robot Co-Manipulation (8.30-8.50)</div>
                <div id="talk1" class="talk-abstract">
                  The talk will focus on how to incorporate human physical behaviour into the robot control loop for adaptive human-robot co-manipulation and co-learning.
                  The human physical behaviour is encoded with several approaches. Higher-level human physical behaviour is learned with a statistical machine learning
                  method to capture the stochasticity in human actions. The learned behaviour is also augmented by human motor control models that provide additional
                  intent prediction capabilities to the collaborating robot by understanding the underlying tradeoffs in human movements. The insights about the
                  lower-level human physical behaviour are incorporated via musculoskeletal models to ensure ergonomics and safety of co-manipulation. Finally,
                  a co-learning method based on reinforcement learning facilitates continuous adaptation of both the robot and the human. For a more direct 
                  transfer of human skill, we will examine the concept of teaching robots through teleimpedance, where the developed interfaces enable a 
                  human operator to (remotely) teach and correct physically interactive tasks.
                </div>
              </div>
              <div class="talk-container">
                <div class="talk-title" onclick="toggleAbstract('talk5')"><b>Harold Soh</b> - Thoughts on Teaching Robots: Diffusion Models, Test-Time Conditioning, and the Path Ahead (8.50-9.10)</div>
                <div id="talk5" class="talk-abstract">
                  In this talk, I will present our recent work on imitation learning using diffusion models, with
                  a focus on test-time conditioning to adapt robot behavior. I will also highlight our use of differentiable
                  model checking to validate behavioral correctness. Time permitting, I hope to open a discussion on the
                  limitations of current approaches—including our own—and the shifts required to advance foundation model-based
                  methods for human-centered robot teaching.
                </div>
              </div>
              <div class="talk-container">
                <div class="talk-title" onclick="toggleAbstract('talk4')"><b>Tapomayukh Bhattacharjee</b> - Towards Robot Caregiving: Building Robots That Work with Humans-in-the-Loop (9.10-9.30)</div>
                <div id="talk4" class="talk-abstract">
                  How can we build robots that meaningfully assist people with mobility limitations in their daily lives?
                  To support complex caregiving tasks such as robot-assisted feeding, transferring, bathing, and meal preparation,
                  robots must physically interact with people and objects in dynamic, unstructured environments where 
                  robot autonomy can fail. In this talk, I will present our work on building human-in-the-loop physical
                  robot caregiving systems that integrate multimodal perception with adaptive algorithms, enabling them
                  to be used by non-expert users and to adapt their strategies to unique user preferences. Together, 
                  these efforts move us closer to creating caregiving robots that are not only technically capable, but 
                  are also responsive to the real needs of people in care settings.
                </div>
              </div>
              <div class="talk-container">
                <div class="talk-title" onclick="toggleAbstract('talk2')"><b>Matthew Gombolay</b> - Teaching the Teachers: Robots Helping Humans Demonstrate for Better Robot Learning (9.30-9.50)</div>
                <div id="talk2" class="talk-abstract">
                  Assistive robots hold a promise of addressing critical societal needs with an aging
                  population, such as supporting older adults with activities of daily living. Yet, current systems are
                  costly, narrowly focused, and unscalable. While large-scale datasets and generalist policies
                  show promise, achieving positive societal impact will require robots to learn directly from
                  diverse, non-expert users, who often lack expertise as robot teachers to provide effective data.
                  In this talk, I will share findings from studies with individuals with Mild Cognitive Impairment
                  (MCI) and their caregivers, offering unique insights into the challenges and design principles for
                  user-driven robot learning. I will also present approaches to help end-users provide better
                  demonstrations, including a reciprocal teaching framework where robots model user behaviors
                  and offer real-time feedback. Finally, I will propose a roadmap for removing roboticists from the
                  loop, enabling scalable, autonomous deployment of cognitive robots in homes.
                </div>
              </div>

            </td>
          </tr>

          <tr>
            <td class="tg-jkyp">9:50 AM</td>
            <td class="tg-jkyp">10.30 AM</td>
            <td class="tg-za14"><b>Paper Presentations: Part 1 </b> </td>
          </tr>
          
          <tr>
            <td class="tg-jkyp">10:30 AM</td>
            <td class="tg-jkyp">11:30 AM</td>
            <td class="tg-za14"><b>Coffee Break & Poster Session</td>
          </tr>

          <tr>
            <td class="tg-jkyp">11:30 AM</td>
            <td class="tg-jkyp">12:30 PM</td>
            <td class="tg-za14"><b>Panel Discussion - Will robotics achieve zero-shot generalization or will we always need to fine-tune with human feedback?</b> <br/> 
            <b>Host:</b> Mike Hagenow <br/>
            <b>Panelists:</b> Harold Soh, Tapomayukh Bhattacharjee, Daniel Brown</td>
          </tr>

          <!-- <tr>
            <td class="tg-jkyp">12:00 AM</td>
            <td class="tg-jkyp">12:30 PM</td>
            <td class="tg-za14"><b>Group Activity I - Debate:</b> Zero-shot Generalization vs Finetuning from Human Feedback <br/> </td>
          </tr> -->

          <tr>
            <td class="tg-jkyp">12:30 PM</td>
            <td class="tg-jkyp">2:30 PM</td>
            <td class="tg-za14"><b>Lunch</td>
          </tr>

          <tr>
            <td class="tg-jkyp">2:30 PM</td>
            <td class="tg-jkyp">4:00 PM</td>
            <td class="tg-za14"><b>Invited Speakers: Part 2 </b><br/>
              <div class="talk-container">
                <div class="talk-title" onclick="toggleAbstract('talk7')"><b>Andreea Bobu</b> - Reading Between the Lines: Using Language Models to Amplify Human Input in Robot Learning (2.30-2.50)</div>
                <div id="talk7" class="talk-abstract">
                  Human-in-the-loop robot learning faces a fundamental data challenge that general machine learning 
                  doesn't: unlike settings where we can collect massive offline datasets, robots must learn from limited,
                  real-time human interactions. This creates a critical bottleneck: we need methods that can make the 
                  most of limited human input, or, in other words, that can learn a lot from a little. The key insight
                  in this talk is that large language models, having been trained on vast amounts of human data, already
                  possess the common sense and semantic priors we need to fill in these gaps. When someone demonstrates
                  a task or gives feedback, there's often implicit information that seems obvious to humans but that
                  robots overlook completely. I discuss three approaches that use language models to "read between the
                  lines" of human input. I demonstrate how LLMs can take sparse human labels and enable robots to
                  generalize to complex expressions, extract hidden preferences that are implied by human behavior
                  but not explicitly stated, and identify missing task concepts based on the situational context of
                  human input. By strategically combining minimal human input with the rich prior knowledge embedded
                  in language models, we can achieve the kind of sample-efficient learning that human-in-the-loop
                  robotics demands for real-world deployment.
                </div>
              </div>
              <div class="talk-container">
                <div class="talk-title" onclick="toggleAbstract('talk3')"><b>Elliott Rouse</b> - TBD (2.50-3.10)</div>
                <div id="talk3" class="talk-abstract">
                  TBD.
                </div>
              </div>
              <div class="talk-container">
                <div class="talk-title" onclick="toggleAbstract('talk6')"><b>Katia Sycara</b> - Human-AI collaboration and goal alignment in planning for dynamic environments (3.10-3.30)</div>
                <div id="talk6" class="talk-abstract">
                  We are interested in collaborative planning between agents that learns and humans in
                  environments that are dynamic and time-stressed. In such a setting, plan generation may
                  involve multiple iterations of human feedback to an AI in order to effectively capture the
                  human's preferences for various plan constraints, or when plans need to be revised . This
                  topic presents two major challenges relevant to this talk. First, Reinforcement Learning,
                  the natural framework for sequential decision making, produces black box policies,
                  making it very challenging (impossible) for humans to interact with it in bidirectional ways.
                  Second, Reinforcement Learning has shown to be vulnerable to mis-specification of the
                  reward function resulting in mis-alignment of the reward with human intended goal(s).
                  Finally, Reinforcement Learning policies determine actions to perform in a given
                  environment state during execution; extracting long-term plans from policies is difficult,
                  especially where environments are highly uncertain, or where a model of environment
                  dynamics is unavailable. To address these challenges we have developed a neuro-
                  symbolic approach where Large Language Models (LLMs) are used to translate user
                  feedback to machine-understandable goal specifications for interpretable interaction
                  between human and agent. Creating suitable machine understandable descriptions of the
                  planning domain, problem, and goal requires expertise in the planning language, limiting
                  the utility of these tools for non-expert humans. Our approach performs initial translation
                  of goal specifications to a set of Planning Domain Definition Language (PDDL) goal
                  constraints using an LLM; such translations often result in imprecise symbolic
                  specifications, which are difficult to validate directly. To address this, we use an
                  evolutionary approach to generate a population of symbolic goal specifications and train a
                  LSTM model to assess whether induced plans in the population adheres to the natural
                  language user goal specifications and feedback. I will present our approach, evaluation
                  results and discuss open challenges.
                </div>
              </div>
              <div class="talk-container">
                <div class="talk-title" onclick="toggleAbstract('talk8')"><b>Kay Ke</b> - TBD (3.30-3.50)</div>
                <div id="talk8" class="talk-abstract">
                  TBD.
                </div>
              </div>
            </td>
          </tr>

          <tr>
            <td class="tg-jkyp">4:00 PM</td>
            <td class="tg-jkyp">5:00 PM</td>
            <td class="tg-za14"><b>Coffee Break & Poster Session</td>
          </tr>

          <tr>
            <td class="tg-jkyp">5:00 PM</td>
            <td class="tg-jkyp">5.40 PM</td>
            <td class="tg-za14"><b>Paper Presentations: Part 2</b></td>
          </tr>

          <tr>
            <td class="tg-jkyp">5:40 PM</td>
            <td class="tg-jkyp">6:30 PM</td>
            <td class="tg-za14"><b>Panel Discussion - Can we replace humans’ feedback with feedback from foundation models?</b> <br/> 
            <b>Host:</b> Erdem Bıyık <br/>
            <b>Panelists:</b> Katia Sycara, Andreea Bobu, Maegan Tucker, Kay Ke </td>
          </tr>

          <!-- <tr>
            <td class="tg-jkyp">6:10 PM</td>
            <td class="tg-jkyp">6:30 PM</td>
            <td class="tg-za14"><b>Group Activity II - Debate:</b> Learning from Real Humans vs Foundation Models <br/> </td>
          </tr> -->

        </tbody>
      </table>

    </div>

    <div class="section" id="speakers">
      <h2>Invited Speakers</h2>
      <!-- <p> listed according to surname </p> -->
      <div class="people">

        <a href="https://sites.google.com/site/tapomayukh">
          <img src="./assets/tapo.jpg">
          <div>Tapomayukh Bhattacharjee</div>
          <div class="aff">Cornell</div>
        </a>

        <a href="https://www.mit.edu/~abobu/">
          <img src="./assets/andrea.png">
          <div>Andreea Bobu</div>
          <div class="aff">MIT</div>
        </a>

        <!-- <a href="https://drazenb.github.io/">
          <img src="./assets/drazen.png">
          <div>Dražen Brščić</div>
          <div class="aff">Kyoto University</div>
        </a> -->

        <a href="https://sites.gatech.edu/matthew-gombolay/">
          <img src="./assets/matthew.png">
          <div>Matthew Gombolay</div>
          <div class="aff">Georgia Tech</div>
        </a>

        <a href="https://kayke.xyz/">
          <img src="./assets/kay.jpg">
          <div>Kay Ke</div>
          <div class="aff">Physical Intelligence (&#960;)</div>
        </a>

      </div>
      <div class="people">        

        <a href="https://www.lukapeternel.nl/">
          <img src="./assets/luka.jpg">
          <div>Luka Peternel</div>
          <div class="aff">TU Delft</div>
        </a>

        <a href="https://kpertsch.github.io//">
          <img src="./assets/pertsch.jpg">
          <div>Karl Pertsch</div>
          <div class="aff">Physical Intelligence (&#960;)</div>
        </a>

        <a href="http://www.elliottjrouse.com/?home">
          <img src="./assets/elliott.jpg">
          <div>Elliott Rouse</div>
          <div class="aff">University of Michigan</div>
        </a>

        <a href="https://haroldsoh.com/">
          <img src="./assets/harold.jpeg">
          <div>Harold Soh</div>
          <div class="aff">National University of Singapore</div>
        </a>

        <a href="https://www.ri.cmu.edu/ri-faculty/katia-sycara/">
          <img src="./assets/katia.png">
          <div>Katia Sycara</div>
          <div class="aff">CMU</div>
        </a>
   
      </div>
    </div>


    <div class="section" id="organizers">
      <h2>Organizers</h2>
      <!-- <p> listed alphabetically </p> -->
      <div class="people">

        <a href="https://ygtkorkmaz.github.io/">
          <img src="./assets/yigit.png">
          <div>Yigit Korkmaz</div>
          <div class="aff">USC</div>
        </a>

        <a href="https://misoshiruseijin.github.io/">
          <img src="./assets/ayano.jpg">
          <div>Ayano Hiranaka</div>
          <div class="aff">USC</div>
        </a>

        <a href="https://aliang8.github.io/">
          <img src="./assets/anthony.jpg">
          <div>Anthony Liang</div>
          <div class="aff">USC</div>
        </a>

        <a href="https://www.hageneaux.com/">
          <img src="./assets/mike.jpg">
          <div>Mike Hagenow</div>
          <div class="aff">MIT</div>
        </a>

      </div>
      <div class="people">

        <a href="https://www.robot.soc.i.kyoto-u.ac.jp/~kanda/index.html">
          <img src="./assets/takayuki.jpg">
          <div>Takayuki Kanda</div>
          <div class="aff">Kyoto University</div>
        </a>

        <a href="https://users.cs.utah.edu/~dsbrown/">
          <img src="./assets/daniel.jpg">
          <div>Daniel Brown</div>
          <div class="aff">University of Utah</div>
        </a>

        <a href="https://maegantucker.com/">
          <img src="./assets/maegan.jpg">
          <div>Maegan Tucker</div>
          <div class="aff">Georgia Tech</div>
        </a>

        <a href="https://ebiyik.github.io/">
          <img src="./assets/erdem.jpg">
          <div>Erdem Bıyık</div>
          <div class="aff">USC</div>
        </a>

      </div>
    </div>

</div>

<div class="foot">
    The website template is borrowed from <a href="https://ai-workshops.github.io/generalizable-policy-learning-in-the-physical-world/">here</a>. 
    <br>
    For inquiries, contact us at: <a href="mailto:hitl.robotlearning@gmail.com">hitl.robotlearning@gmail.com</a>
</div>

<script>
function toggleAbstract(talkId) {
  const abstract = document.getElementById(talkId);
  const title = abstract.previousElementSibling;
  abstract.classList.toggle('show');
  title.classList.toggle('expanded');
}
</script>
</body>
</html>
